{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6527d5d-c539-4b81-aa51-1a90fce4b1f4",
   "metadata": {},
   "source": [
    "# Notebook Purpose\n",
    "\n",
    "This notebook is what I used to create the dataset:\n",
    "`dleemiller/irish_penny_journal`\n",
    "\n",
    "[huggingface link](https://huggingface.co/datasets/dleemiller/irish_penny_journal)\n",
    "\n",
    "It it chunks and cleans the source text and tries to remove artifacts. Then it uses a basic dspy pipeline to process the text into cleaned and modernized text representations. These are saved progressively as json files, and then converted to parquet once the processing is completed.\n",
    "\n",
    "This dataset is used to train a classifier. Because the original and modernized texts have the same general meaning, it provides a nice balanced dataset for the style and lexicon to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515fd78-1fed-4ca4-b819-4b729018f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import dspy\n",
    "import random\n",
    "\n",
    "# If you use OpenRouter, create a .env file or change this\n",
    "# to supply it directly\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"APIKEY\"]\n",
    "\n",
    "## Model Rotation\n",
    "## Pick the ones you want or change them\n",
    "## You can also use other sources like ollama or whatever\n",
    "## litellm supports\n",
    "\n",
    "#BASE_MODEL1 = \"openrouter/google/gemini-2.5-flash-preview\"\n",
    "BASE_MODEL2 = \"openrouter/google/gemini-2.0-flash-001\"\n",
    "BASE_MODEL3 = \"openrouter/meta-llama/llama-4-maverick\"\n",
    "BASE_MODEL4 = \"openrouter/qwen/qwen3-235b-a22b\"\n",
    "BASE_MODEL5 = \"openrouter/meta-llama/llama-3.3-70b-instruct\"\n",
    "\n",
    "BASE_MODELS = [BASE_MODEL2, BASE_MODEL3, BASE_MODEL4, BASE_MODEL5]\n",
    "\n",
    "lm_list = [dspy.LM(x, api_key=api_key, temperature=0.8, cache=False, max_tokens=16000) for x in BASE_MODELS]\n",
    "dspy.configure(lm=lm_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff89739-1c32-49ca-917f-4c4b40fa96c4",
   "metadata": {},
   "source": [
    "# DSPy pipeline\n",
    "\n",
    "Clean and modernize the text, given input from IPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3b16b-94f6-46ad-acca-0cac761b7882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrishPennyTranslator(dspy.Signature):\n",
    "    input_text: str = dspy.InputField(description=\"Text from Irish Penny Journal\")\n",
    "    cleaned_input_text: str = dspy.OutputField(description=\"Cleaned original text\")\n",
    "    modernized_text: str = dspy.OutputField(description=\"Translation to modern (US) English\")\n",
    "\n",
    "IrishPennyTranslator.__doc__ = \"\"\"\n",
    "You are given a passage of text from the Irish Penny Journal (ca 1840)\n",
    "\n",
    "Clean the original text (fix whitespace or artifacts from OCR) or formatting issues.\n",
    "Remove any references to images, pages from the paper. Only retain the prose content.\n",
    "\n",
    "Then your task is to read the passage, and convert it to modern US English.\n",
    "\n",
    "In order to do this:\n",
    "- attempt to retain the original meaning\n",
    "- rephrase in modern English\n",
    "- restructure sentences to modern grammar\n",
    "- apply or translate to modern concepts (eg telegraph -> iphone)\n",
    "\n",
    "/no_think\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9c829-e6dd-4269-835d-06c75032d863",
   "metadata": {},
   "source": [
    "Refine pipeline to retry if not cleaned text has enough similarity to the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b2814-3b1f-4bc2-a0b5-cd480de9eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "def reward_fn(text, pred):\n",
    "    return textdistance.jaccard.similarity(text[\"input_text\"], pred.cleaned_input_text)\n",
    "\n",
    "penny = dspy.ChainOfThought(IrishPennyTranslator)\n",
    "best_of_3 = dspy.Refine(module=penny, N=3, reward_fn=reward_fn, threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e059d6-84fa-45c6-ade9-7b488ace3b89",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "A few generated scripts for processing the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38ca78-6c32-4ae2-8046-4fcb8faf962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clean import extract_penny_journal_segments, analyze_segments, print_sample_segments, save_segments_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc919e7b-1040-410c-951b-0bfd049109a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "texts = []\n",
    "\n",
    "for filename in glob.glob(\"./original_texts/*.txt\"):\n",
    "    with open(filename, \"r\") as fh:\n",
    "        texts.append(fh.read())\n",
    "\n",
    "print(\"üîç Processing Irish Penny Journal with strict quality filters...\")\n",
    "\n",
    "# Extract high-quality segments\n",
    "segments = []\n",
    "for text in texts:\n",
    "    segments += extract_penny_journal_segments(text, min_words=50, max_words=150)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(segments)} high-quality prose segments\")\n",
    "\n",
    "# Analyze results\n",
    "stats = analyze_segments(segments)\n",
    "\n",
    "print(f\"\\nüìä Quality Analysis:\")\n",
    "print(f\"   Total segments: {stats['total_segments']}\")\n",
    "print(f\"   Total words: {stats['total_words']:,}\")\n",
    "print(f\"   Average words per segment: {stats['avg_words_per_segment']:.1f}\")\n",
    "print(f\"   Word count range: {stats['min_words']} - {stats['max_words']}\")\n",
    "\n",
    "# Distribution analysis\n",
    "dist = stats['word_count_distribution']\n",
    "print(f\"\\nüìà Word Count Distribution:\")\n",
    "print(f\"   Under 50 words: {dist['under_50']} segments\")\n",
    "print(f\"   50-100 words: {dist['50_to_100']} segments\")\n",
    "print(f\"   100-150 words: {dist['100_to_150']} segments\")\n",
    "print(f\"   Over 150 words: {dist['over_150']} segments\")\n",
    "\n",
    "# Quality validation\n",
    "if dist['under_50'] == 0:\n",
    "    print(f\"   ‚úÖ Perfect: No segments under minimum threshold\")\n",
    "elif dist['under_50'] < len(segments) * 0.05:\n",
    "    print(f\"   ‚úÖ Excellent: Less than 5% under threshold\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: {dist['under_50']} segments under 50 words\")\n",
    "\n",
    "# Show sample segments\n",
    "print_sample_segments(segments)\n",
    "\n",
    "# Save to file\n",
    "save_segments_to_file(segments)\n",
    "print(f\"\\nüíæ Segments saved to 'penny_journal_segments.txt'\")\n",
    "\n",
    "# Projection for full dataset\n",
    "print(f\"\\nüîÆ Measurement for 52 issues:\")\n",
    "projected_segments = len(segments)\n",
    "projected_words = stats['total_words']\n",
    "\n",
    "print(f\"   Segments: {projected_segments:,}\")\n",
    "print(f\"   Words: {projected_words:,}\")\n",
    "\n",
    "if projected_segments >= 2500:\n",
    "    print(f\"   ‚úÖ Excellent dataset size for style classification!\")\n",
    "elif projected_segments >= 1500:\n",
    "    print(f\"   ‚úÖ Good dataset size for style classification\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Consider adding other penny journals for larger dataset\")\n",
    "\n",
    "print(f\"\\nüéØ This dataset should work excellently for training your penny journal style classifier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6a428-7a7e-434d-b96d-b36036ab770c",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Check the pipeline works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746874e-120d-4363-8253-573ec5446f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_of_3(input_text=segments[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8589c36-de04-4e15-b901-f83ac894a55b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Generate the dataset as individual json files. Hash the text and use it as a filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef63cd4-a133-41a7-85ca-adcb0f2e55f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "texts = [x[\"text\"] for x in segments]\n",
    "ids = [hashlib.md5(x.encode()).hexdigest() for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e6cef-b5bb-474a-bad8-0d2c99bfbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "\n",
    "JSON_DIR = f\"{DATA_DIR}/json/\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "def hash_filename(text):\n",
    "    \"\"\"\n",
    "    Hash the text to use as a filename.\n",
    "    \"\"\"\n",
    "    text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "    return os.path.join(JSON_DIR, f\"{text_hash}.json\")\n",
    "\n",
    "def save_json(idx, model, text, pred):\n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"cleaned_text\": pred.cleaned_input_text,\n",
    "        \"modernized_text\": pred.modernized_text,\n",
    "        \"thinking\": pred.reasoning,\n",
    "        \"generator_model\": model,\n",
    "        \"id\": idx,\n",
    "        \"source\": \"Irish Penny Journal\",\n",
    "        \"jaccard_similarity\":  textdistance.jaccard.similarity(text, pred.cleaned_input_text)\n",
    "        \n",
    "    }\n",
    "    with open(hash_filename(text), \"w\") as fh:\n",
    "        json.dump(data, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d779d67-c6d4-4569-afe8-453afcb32b25",
   "metadata": {},
   "source": [
    "## Data Generation Loop\n",
    "\n",
    "This step takes a while. This could probably be parallelized with something like `dspy.Parallel`... Or you can just let it go overnight :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35937aa-ede8-41d7-859c-f1ea73eb4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,text in enumerate(texts):\n",
    "    if os.path.exists(hash_filename(text)):\n",
    "        print(f\"Skipping... {hash_filename(text)}\")\n",
    "        continue\n",
    "\n",
    "    l = random.choice(range(len(BASE_MODELS)))\n",
    "    print(f\"Setting {BASE_MODELS[l]}\")\n",
    "    best_of_3.set_lm(lm_list[l])\n",
    "\n",
    "    pred = best_of_3(input_text=text)\n",
    "    print(pred)\n",
    "    save_json(ids[i], BASE_MODELS[l], text, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d12eb5-91e3-4671-98b0-c5c2d0ef597a",
   "metadata": {},
   "source": [
    "# Create parquet\n",
    "\n",
    "Load the json and convert it to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426139b9-2010-4a88-970b-f46355202922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "records = []\n",
    "for file in glob.glob(os.path.join(DATA_DIR, \"*.json\")):\n",
    "    with open(file, \"r\") as fh:\n",
    "        records.append(json.load(fh))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df = df.sample(frac=1).reset_index(drop=True).drop(columns=[\"thinking\"])\n",
    "df.generator_model = df.generator_model.apply(lambda x: x.lstrip(\"openrouter/\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b08ef-9f96-4bc9-8a81-04130c7ba351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"text\", \"cleaned_text\", \"modernized_text\", \"generator_model\", \"jaccard_similarity\", \"id\", \"source\"]].to_parquet(\"irish-penny.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317ba05e-c16e-4053-b02b-c4704f35d5be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
